{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Report \n",
    "You will be required to submit a project report along with your modified agent code as part of your submission. As you complete the tasks below, include thorough, detailed answers to each question provided in italics.\n",
    "\n",
    "## Implement a Basic Driving Agent\n",
    "To begin, your only task is to get the smartcab to move around in the environment. At this point, you will not be concerned with any sort of optimal driving policy. Note that the driving agent is given the following information at each intersection:\n",
    "\n",
    "- The next waypoint location relative to its current location and heading.\n",
    "- The state of the traffic light at the intersection and the presence of oncoming vehicles from other directions.\n",
    "- The current time left from the allotted deadline.\n",
    "\n",
    "To complete this task, simply have your driving agent choose a random action from the set of possible actions (```None```,```'forward'```,```'left'```, ```'right'```) at each intersection, disregarding the input information above. Set the simulation deadline enforcement, ```enforce_deadline``` to ```False``` and observe how it performs.\n",
    "\n",
    "**QUESTION**: Observe what you see with the agent's behavior as it takes random actions. Does the smartcab eventually make it to the destination? Are there any other interesting observations to note?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *ANSWER*: \n",
    "\n",
    "> The smartcab does not make it to the destination. Actions are being taken randomly so there is no learning of the best actions to take and no policy is being applied. Hence there is no need to end up with the route, in average the smartcab gains more if it keeps on moving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inform the Driving Agent\n",
    "Now that your driving agent is capable of moving around in the environment, your next task is to identify a set of states that are appropriate for modeling the smartcab and environment. The main source of state variables are the current inputs at the intersection, but not all may require representation. You may choose to explicitly define states, or use some combination of inputs as an implicit state. At each time step, process the inputs and update the agent's current state using the ```self.state``` variable. Continue with the simulation deadline enforcement ```enforce_deadline``` being set to ```False```, and observe how your driving agent now reports the change in state as the simulation progresses.\n",
    "\n",
    "**QUESTION**: What states have you identified that are appropriate for modeling the smartcab and environment? Why do you believe each of these states to be appropriate for this problem?\n",
    "\n",
    "**OPTIONAL**: How many states in total exist for the smartcab in this environment? Does this number seem reasonable given that the goal of Q-Learning is to learn and make informed decisions about each state? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*ANSWER*: \n",
    "\n",
    "> The current state is defined by the possible combinations of: **light** ('red', 'green'), **oncoming**  (None, 'forward', 'left', 'right'), **right**   (None, 'forward', 'left', 'right'), **left**  (None, 'forward', 'left', 'right') and **next_waypoint** (None, 'forward', 'left', 'right'). \n",
    "\n",
    "> According to the circulation rules I have defined the following states:\n",
    "\n",
    "|   \t| next_waypoint \t|  light  \t|      oncoming      \t| left \t|         right         \t|\n",
    "|:-:\t|:-------------:\t|:-------:\t|:------------------:\t|:----:\t|:---------------------:\t|\n",
    "| 0 \t|   'forward'   \t| 'green' \t|                    \t|      \t|                       \t|\n",
    "| 1 \t|   'forward'   \t|  'red'  \t|                    \t|      \t|                       \t|\n",
    "| 2 \t|     'left'    \t| 'green' \t|    None, 'left'    \t|      \t|                       \t|\n",
    "| 3 \t|     'left'    \t| 'green' \t| 'right', 'forward' \t|      \t|                       \t|\n",
    "|   \t|     'left'    \t|  'red'  \t|                    \t|      \t|                       \t|\n",
    "| 4 \t|    'right'    \t| 'green' \t|                    \t|      \t|                       \t|\n",
    "|   \t|    'right'    \t|         \t|                    \t|      \t| None, 'left', 'right' \t|\n",
    "| 5 \t|    'right'    \t|  'red'  \t|                    \t|      \t|                       \t|\n",
    "|   \t|    'right'    \t|         \t|                    \t|      \t|       'forward'       \t|\n",
    "\n",
    "> I believe these states are appropiate for this problem since they can describe all the possible situations occurring at each intersection, taking into account both the input variables (lights and traffic) and the direction in which we hare heading next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a Q-Learning Driving Agent\n",
    "With your driving agent being capable of interpreting the input information and having a mapping of environmental states, your next task is to implement the Q-Learning algorithm for your driving agent to choose the best action at each time step, based on the Q-values for the current state and action. Each action taken by the smartcab will produce a reward which depends on the state of the environment. The Q-Learning driving agent will need to consider these rewards when updating the Q-values. Once implemented, set the simulation deadline enforcement enforce_deadline to True. Run the simulation and observe how the smartcab moves about the environment in each trial.\n",
    "\n",
    "The formulas for updating Q-values can be found in this video.\n",
    "\n",
    "**QUESTION**: What changes do you notice in the agent's behavior when compared to the basic driving agent when random actions were always taken? Why is this behavior occurring?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *ANSWER*: \n",
    "\n",
    ">  Random actions gave erratic movements of the car and when a policy is applied these movements are more structured. This is happening because the system is learning and the action is being selected following the policy, which takes into account the reward for the action and the Q."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the Q-Learning Driving Agent\n",
    "Your final task for this project is to enhance your driving agent so that, after sufficient training, the smartcab is able to reach the destination within the allotted time safely and efficiently. Parameters in the Q-Learning algorithm, such as the learning rate (alpha), the discount factor (gamma) and the exploration rate (epsilon) all contribute to the driving agent’s ability to learn the best action for each state. To improve on the success of your smartcab:\n",
    "\n",
    "- Set the number of trials, n_trials, in the simulation to 100.\n",
    "- Run the simulation with the deadline enforcement enforce_deadline set to True (you will need to reduce the update delay update_delay and set the display to False).\n",
    "- Observe the driving agent’s learning and smartcab’s success rate, particularly during the later trials.\n",
    "- Adjust one or several of the above parameters and iterate this process.\n",
    "\n",
    "This task is complete once you have arrived at what you determine is the best combination of parameters required for your driving agent to learn successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**: Report the different values for the parameters tuned in your basic implementation of Q-Learning. For which set of parameters does the agent perform best? How well does the final driving agent perform?\n",
    "\n",
    "> *ANSWER*:\n",
    "\n",
    "> I have changed the values of the learning rate (alpha), the discount factor (gamma) and the exploration rate (epsilon) and seen how the smartcab performed, analysing the success rate (how many of the rides ended before deadline) and the number of infractions committed (ilegal movements not following traffic rules).\n",
    "\n",
    ">Simulations are for 100 trials.\n",
    "\n",
    ">For the simulations 1, 2, 3 I only increased the learning rate (alpha). It can be seen that the success rate increases as the learning rate does, since the cab learn faster and more rides end before deadline. The number of infractions, is also reduced with faster learning.\n",
    "\n",
    ">For simulations 4 and 5, the discount factor (gamma) was changed. The discount factor determines how much importance is given to the past experience (Q) in order to take the next action. This means, if the discount factor is bigger, less importance is given to the present reward but more to the previous ones trough the utility. Hence, the number of infractions will be reduced ( previous infractions have given negative rewards and we have learned from that!).\n",
    "\n",
    ">If we change the exploration rate (simulations 6 and 7), we are adding randomness to the following of the policy. This means, higher exploration rates (epsilon) will be reflected in higher number of infractions, since we will not be following the rules that maximize the reward due to the randomness introduced.\n",
    "\n",
    ">|   \t| learning rate  (alpha) \t| discount factor (gamma) \t| exploration rate  (epsilon) \t| succsess rate (num. of susccess /num. of trials) \t| infractions \t|\n",
    "|---\t|:----------------------:\t|:-----------------------:\t|:---------------------------:\t|:------------------------------------------------:\t|:-----------:\t|\n",
    "| 1 \t|           0.2          \t|           0.5           \t|             0.01            \t|                        88%                       \t|      99     \t|\n",
    "| 2 \t|           0.5          \t|           0.5           \t|             0.01            \t|                        92%                       \t|      90     \t|\n",
    "| 3 \t|          0.95          \t|           0.5           \t|             0.01            \t|                        96%                       \t|      81     \t|\n",
    "| 4 \t|           0.5          \t|           0.2           \t|             0.01            \t|                        80%                       \t|      58     \t|\n",
    "| 5 \t|           0.5          \t|           0.95          \t|             0.01            \t|                        43%                       \t|      34     \t|\n",
    "| 6 \t|           0.5          \t|           0.5           \t|             0.2             \t|                        73%                       \t|     238     \t|\n",
    "| 7 \t|           0.5          \t|           0.5           \t|             0.5             \t|                        33%                       \t|     442     \t|\n",
    "\n",
    ">The perfect agent will have intermediate learning rate (to give time to learn and reduce infractions), intermediate discount factor (to give importance both to present reward and previous experience  (Q)) and low exploration rate. I will set this to Longer simulations could be launched in order to have more samples and reduce bias. \n",
    "\n",
    "**QUESTION**: Does your agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, and not incur any penalties? How would you describe an optimal policy for this problem?\n",
    "\n",
    "> *ANSWER*:\n",
    "I think it is close to the optimal policy since the success rate is quite high (above 90%), but longer simulations can be taken to reduce the number of infractions.\n",
    "\n",
    "> I repeated the simulations for 250 trials and the next are the results obtained:\n",
    "\n",
    ">|   \t| learning rate  (alpha) \t| discount factor (gamma) \t| exploration rate  (epsilon) \t| succsess rate (num. of susccess /num. of trials) \t| infractions \t|\n",
    "|---\t|:----------------------:\t|:-----------------------:\t|:---------------------------:\t|:------------------------------------------------:\t|:-----------:\t|\n",
    "| 8 \t|           0.2          \t|           0.5           \t|             0.01            \t|                        92.4%                       \t|      200     \t|\n",
    "| 9 \t|           0.5          \t|           0.5           \t|             0.01            \t|                        96.4%                       \t|      162     \t|\n",
    "| 10 \t|          0.95          \t|           0.5           \t|             0.01            \t|                        92%                       \t|      194     \t|\n",
    "|11 \t|           0.95          \t|           0.2           \t|             0.01            \t|                        98.4%                       \t|      183     \t|\n",
    "| 12 \t|           0.95          \t|           0.95          \t|             0.01            \t|                        73.2%                       \t|      293     \t|\n",
    "\n",
    ">Given the results I would choose either $\\alpha=0.5$, $\\gamma= 0.5$ (simulation 9) and  $\\epsilon=0.01$ or  $\\alpha=0.95$, $\\gamma= 0.95$ and  $\\epsilon=0.01$ (simulation 11).\n",
    "\n",
    ">In my opinion  $\\alpha=0.5$, $\\gamma= 0.5$ and  $\\epsilon=0.01$ (simulation 11) is better since it gives importance both to the rewards and past utility (Q) and give high success rate (96.4%) and moderate number of infractions (162)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
