<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>capstone_report_mabelvj</title><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 .s1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 17pt; }
 h1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 14pt; }
 .p, p { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; margin:0pt; }
 .s2 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt; }
 .s3 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 h2 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 .s4 { color: #EC008C; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .a { color: #00F; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s5 { color: #00F; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s6 { color: #EC008C; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .h3, h3 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .s7 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s8 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s9 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 4pt; }
 .s10 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 2pt; }
 .s12 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 1pt; }
 .s13 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l1 {padding-left: 0pt;counter-reset: c1 0; }
 #l1> li:before {counter-increment: c1; content: counter(c1, upper-roman)". "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 14pt; }
 #l2 {padding-left: 0pt;counter-reset: c2 0; }
 #l2> li:before {counter-increment: c2; content: counter(c1, upper-roman)"."counter(c2, upper-roman)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 #l3 {padding-left: 0pt;counter-reset: d1 0; }
 #l3> li:before {counter-increment: d1; content: counter(d1, decimal)". "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l4 {padding-left: 0pt; }
 #l4> li:before {content: "• "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l5 {padding-left: 0pt;counter-reset: c2 0; }
 #l5> li:before {counter-increment: c2; content: counter(c1, upper-roman)"."counter(c2, upper-roman)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 #l6 {padding-left: 0pt;counter-reset: c3 0; }
 #l6> li:before {counter-increment: c3; content: "("counter(c3, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 #l7 {padding-left: 0pt; }
 #l7> li:before {content: "• "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l8 {padding-left: 0pt; }
 #l8> li:before {content: "• "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l9 {padding-left: 0pt;counter-reset: g1 0; }
 #l9> li:before {counter-increment: g1; content: counter(g1, decimal)". "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l10 {padding-left: 0pt;counter-reset: c2 0; }
 #l10> li:before {counter-increment: c2; content: counter(c1, upper-roman)"."counter(c2, upper-roman)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 #l11 {padding-left: 0pt; }
 #l11> li:before {content: "• "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l12 {padding-left: 0pt;counter-reset: c2 0; }
 #l12> li:before {counter-increment: c2; content: counter(c1, upper-roman)"."counter(c2, upper-roman)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 #l13 {padding-left: 0pt;counter-reset: c3 0; }
 #l13> li:before {counter-increment: c3; content: "("counter(c3, lower-latin)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 #l14 {padding-left: 0pt;counter-reset: c2 0; }
 #l14> li:before {counter-increment: c2; content: counter(c1, upper-roman)"."counter(c2, upper-roman)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
</style></head><body><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 2pt;padding-left: 98pt;text-indent: 0pt;text-align: center;">Capstone Project:</p><p class="s1" style="padding-top: 2pt;padding-left: 98pt;text-indent: 0pt;line-height: 112%;text-align: center;">Right Whale call recognition using Convolutional Neural Networks</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">Machine Learning Engineer Nanodegree</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Isabel María Villalba Jiménez Thursday 3<span class="s2">rd</span><span class="s3">  </span>November, 2016</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l1"><li style="padding-left: 32pt;text-indent: -26pt;text-align: justify;"><h1 style="display: inline;"><a name="bookmark0">Definition</a></h1><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l2"><li style="padding-left: 33pt;text-indent: -27pt;text-align: justify;"><h2 style="display: inline;"><a name="bookmark1">Project Overview</a></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">Right whales are one of the most endangered species around the world, with only a few 400 remaining. Many of casualties among them are caused by crashing into boats. One way of avoiding these collisions is to alert ships when whales are detected in the proximity.</p><p class="s6" style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark4" class="s13">In order to do so, Cornell University’s Bioacoustic Research Program, which has extensive experience in identifying endangered whale species, has deployed a 24/7 buoy network to guide ships from colliding with the world’s last 400 North Atlantic right whales (see figure </a>1<span style=" color: #000;">).</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 90pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="340" height="208" alt="image" src="capstone_report_mabelvj/Image_001.jpg"/></span></p><p class="s5" style="padding-top: 8pt;padding-left: 5pt;text-indent: 2pt;text-align: justify;"><a href="#bookmark43" class="s13" name="bookmark4">Figure 1: Illustration of the deployment of the buoys in the sea while coexisting with whales [</a>1<span style=" color: #000;">]</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="https://www.kaggle.com/c/whale-detection-challenge" class="s13" target="_blank">This work comes from a proposal of the Cornell University’s Bioacoustic Research Program of finding new ways of improving the detection of these mammals through the audio signal of the buoys network. Cornell University provides for the competition a dataset with recordigs made by the buoys. The proposal was made through a Kaggle competition named </a><a href="https://www.kaggle.com/c/whale-detection-challenge" class="a" target="_blank">The Marinexplore and Cornell University Whale Detection </a>Challenge <a href="#bookmark43" class="s13">[</a>1<span style=" color: #000;">] “Copyright © 2011 by Cornell University and the Cornell Research Foundation, Inc. All Rights Reserved”.</span></p></li><li style="padding-top: 2pt;padding-left: 38pt;text-indent: -32pt;text-align: justify;"><h2 style="display: inline;"><a name="bookmark2">Problem Statement</a></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark5" class="s13">Right whales make a half-dozen types of sounds, but the most characteristic one is the up-call. This type of &quot;contact call&quot;, is a little like small talk– the sound of a right whale going about its day and letting others know it is nearby. In figure </a><span style=" color: #EC008C;">2 </span><a href="#bookmark44" class="s13">it is represented the spectrogram of an up-call which sounds like a deep, rising “whoop” that lasts about a second (sound in [</a>2<a href="#bookmark45" class="s13">], other calls in [</a>3<span style=" color: #000;">]).</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 154pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="170" height="170" alt="image" src="capstone_report_mabelvj/Image_002.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 95pt;text-indent: 0pt;text-align: center;"><a href="#bookmark44" class="s13" name="bookmark5">Figure 2: Spectrogram of a rigth whale up-call [</a>2<span style=" color: #000;">]</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">The goal of this work is to present a model capable of detecting the right whale’s up-call, which is the most characteristic call of this specie, from the audio detected by the buoys deployed in the sea.</p><p class="s5" style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="https://www.kaggle.com/c/noaa-right-whale-recognition" class="s13" target="_blank">Impressed by the working principle of Convolutional Neural Networks, I decided looking for uses beyond pure image classification. I also had been wondering if anything related to animals and whales could be done. I started looking in the internet and found several Kaggle competitions: one on whale detection through images (</a><a href="https://www.kaggle.com/c/noaa-right-whale-recognition" class="a" target="_blank">Right Whale </a>Recognition<a href="https://www.kaggle.com/c/whale-detection-challenge" class="s13" target="_blank">), and other on recognizing the North Atlantic Right Whale call (</a><a href="https://www.kaggle.com/c/whale-detection-challenge" class="a" target="_blank">The Marinexplore and Cornell University Whale Detection </a>Challenge<a href="#bookmark46" class="s13">). Searching for applications of Convolutional Neural Networks in sound recognition, I found an entry related to the The Marinexplore and Cornell University Whale Detection Challenge. In [</a>4<span style=" color: #000;">] Daniel Nouri proposed to use ConvNets not just to go across the spectrogram of the whale calls, but try to recognize a pattern by simply looking at its image, like a human could. With this proposal he got pretty good results with a very straight forward approach. I decided to give it a try and look for most used ConvNets schemes and see their performance in this competition.</span></p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">The <b>workflow </b>can be organized as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l3"><li style="padding-left: 30pt;text-indent: -12pt;text-align: left;"><p style="display: inline;">sound samples exploration</p></li><li style="padding-top: 8pt;padding-left: 30pt;text-indent: -12pt;text-align: left;"><p style="display: inline;">spectrogram generation and image processing (contrast, appropriate dimensions...)</p></li><li style="padding-top: 8pt;padding-left: 30pt;text-indent: -12pt;text-align: left;"><p style="display: inline;">separation of dataset into training, cross-validation and test dataset and save into pickle</p></li><li style="padding-top: 8pt;padding-left: 30pt;text-indent: -12pt;text-align: left;"><p style="display: inline;">select ConvNets model and adjust parameters</p><ul id="l4"><li style="padding-top: 8pt;padding-left: 52pt;text-indent: -12pt;text-align: left;"><p style="display: inline;">define structure of the ConvNet adequate for the images: depth of layers, and stride and patch size of filters and pooling layers</p></li><li style="padding-top: 4pt;padding-left: 52pt;text-indent: -12pt;text-align: left;"><p style="display: inline;">AUC vs epochs (or training iterations), Error vs epochs, for different batch sizes</p></li><li style="padding-top: 4pt;padding-left: 52pt;text-indent: -12pt;text-align: left;"><p style="display: inline;">tune the model using regularization and decaying learning rate</p></li></ul></li><li style="padding-top: 8pt;padding-left: 30pt;text-indent: -12pt;text-align: left;"><p style="display: inline;">compare the performance of winning model using the reduced version train and test dataset extracted from the train dataset</p></li></ol></li><li style="padding-top: 2pt;padding-left: 43pt;text-indent: -37pt;text-align: justify;"><h2 style="display: inline;"><a name="bookmark3">Metrics</a><a name="bookmark6">&zwnj;</a></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">The main evaluation metric for this project will be that used in the Kaggle competition, this is the</p><h3 style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">Area Under the Curve (AUC)<span class="p">, where the Curve is the ROC curve.</span></h3><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">The <b>receiver operating characteristic (ROC) </b>curve is a graphical plot that illustrates the performance of a binary classifier system as its discrimination threshold is varied. The curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.</p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">The true-positive rate is also known as sensitivity, recall or probability of detection. The false-positive rate is also known as the fall-out or probability of false alarm.</p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">The ROC curve is thus, the sensitivity as a function of fall-out. In general, if the probability distributions for both detection and false alarm are known, the ROC curve can be generated by plotting the cumulative distribution function (area under the probability distribution from <i>−∞ </i><a href="#bookmark7" class="s13">to the discrimination threshold) of the detection probability in the y-axis versus the cumulative distribution function of the false-alarm probability in x-axis (see figure </a><span style=" color: #EC008C;">3</span><a href="#bookmark47" class="s13">)[</a><span style=" color: #00F;">5</span>].</p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">Other interesting tool can be the <b>confusion matrix</b><a href="#bookmark47" class="s13">, which is a more detailed version of the ROC curve. The confusion matrix is a table that shows the predicted labels for each of the true input labels. Hence, this table shoes true positives (TPs), false positives (FPs), true negatives (TNs) and false negatives (FNs). Each prediction result or instance of the confusion matrix represents a point in the ROC space [</a><span style=" color: #00F;">5</span>].</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 133pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="228" height="371" alt="image" src="capstone_report_mabelvj/Image_003.jpg"/></span></p><p class="s5" style="padding-top: 8pt;padding-left: 95pt;text-indent: 0pt;text-align: center;"><a href="#bookmark48" class="s13" name="bookmark7">Figure 3: ROC curve graphic explanation [</a>6<span style=" color: #000;">]</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><span style=" color: #000;">Another important measure can be the </span><span class="h3">error rate </span><a href="#bookmark49" class="s13">vs iterations for different batch sizes. The error rate used can be the percentage of wrong classified samples. This metric is widely used in literature for ConvNets [</a>7<a href="#bookmark50" class="s13">, </a>8<a href="#bookmark49" class="s13">] as it gives an insight of the state of training of the network. It is quite common to detect over-training. When this phenomenon occurs, the training error keeps decreasing over time, but the test error goes through a minimum and then starts increasing after a certain number of iteration [</a>7<span style=" color: #000;">].</span></p></li></ol></li><li style="padding-top: 2pt;padding-left: 38pt;text-indent: -33pt;text-align: justify;"><h1 style="display: inline;"><a name="bookmark8">Analysis</a></h1><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l5"><li style="padding-left: 38pt;text-indent: -32pt;text-align: justify;"><h2 style="display: inline;"><a name="bookmark9">Data Exploration</a><a name="bookmark13">&zwnj;</a></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark44" class="s13">The dataset used comes from the competition and consists of 30,000 training samples and 54,503 testing samples. Each candidate is a 2-second .aiff sound clip with a sample rate of 2 kHz. The file &quot;train.csv&quot; gives the labels for the train set. Candidates that contain a right whale call have label=1 otherwise, label=0. These clips contain any mixture of right whale calls, non-biological noise, or other whale calls [</a>2<a href="#bookmark45" class="s13">, </a>3<span style=" color: #000;">].</span></p><p class="s6" style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark14" class="s13">The training dataset is imbalanced, consisting of 7027 right whale up-calls samples (label=1) and 22973 non right whale up-calls samples (label=0) (figure </a>4a<span style=" color: #000;">), with a total of 30000 samples.</span></p><p class="s6" style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark35" class="s13">Test labels are not available in the dataset file and Kaggle will not make it available since the competition is closed. As it will be explained later in the section Model Evaluation and Validation </a>IV.I<a href="#bookmark14" class="s13">, the number of samples in the training dataset are not enough to provide good amount of data for training, validation and testing. I decided to obtain the labels using winning models and extend the amount of data available for training. Data distribution is presented in figure </a>4b<span style=" color: #000;">. The test dataset is also imbalanced, with 13276 right whale up-calls samples (label=1) and 41227 non up-calls samples (label=0), with a total of 54503 samples.</span></p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">As it can be seen, the training and test dataset have more or less the same proportion of data from each label. However, both dataset are quite imbalanced. In order to deal with this problem, I could just balance the samples used (get the same amount of labels from each type) or try to use an algorithm that penalizes this imbalance.</p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">The dataset split will be as follows: <b>training dataset </b>is split into training and validation dataset, with respectively, 90% and 10% of the training data, which means 12648 and 1406 samples from the balanced training dataset. This 90-10 split, instead of the usual 80-20 split, is due to the lack of data in the training dataset, what makes to put the efforts in maximizing the amount of data for training. Data from the extended dataset could have been added, but I preferred to keep the training to the true training samples and labels, and not to give any mislabeled data in this process. For testing, labels obtained from the simulation will be used. However, the whole <b>test dataset </b>is too big to evaluate in iterations and it will be reduced to 10000 random samples from the 54503 test samples.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="282" height="194" alt="image" src="capstone_report_mabelvj/Image_004.jpg"/></span>	<span><img width="282" height="194" alt="image" src="capstone_report_mabelvj/Image_005.jpg"/></span></p><ol id="l6"><li style="padding-top: 5pt;padding-left: 82pt;text-indent: -14pt;text-align: center;"><p class="s8" style="display: inline;"><a name="bookmark14">Training dataset                                                        (b) Test dataset</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: center;">Figure 4</p></li></ol></li><li style="padding-top: 2pt;padding-left: 43pt;text-indent: -37pt;text-align: justify;"><h2 style="display: inline;"><a name="bookmark10">Exploratory  Visualization</a></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark51" class="s13">The audio is recorded after the buoys auto-detect the characteristic up-call, what biases the dataset to that kind of calls. Thus, it makes sense to detect only the up-call, which is the most characteristic and most frequently emitted call (details on the deployment of the buoys and how the recordings are made in [</a>9<span style=" color: #000;">]).</span></p><p class="s6" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark15" class="s13">In figure </a>5<a href="#bookmark5" class="s13">, samples corresponding to right-whale up-call (label = 1) show the that the energy of the signal is below 250Hz (see figure </a>2<a href="#bookmark15" class="s13">) and they exhibit a clear pattern. This fact will help to reduce information processed to that range of frequencies. In the second row, corresponding to negative identifications (label = 0) this pattern is not present. However, some samples from other whale species or corresponding to right-whale making other calls are included in the label 0. This could be the case of sample train6776 in figure </a>5<span style=" color: #000;">.</span></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">Recordings have a duration of 1.8 seconds and a sampling rate of 2000 Hz. The raw spectrogram of the recordings results in images of 129x23 pixels.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 48pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="457" height="343" alt="image" src="capstone_report_mabelvj/Image_006.jpg"/></span></p><p style="padding-top: 7pt;padding-left: 25pt;text-indent: 0pt;text-align: left;"><a name="bookmark15">Figure 5: Samples for right whale up-call (label 1) and no-right-whale up-call (label 0).</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 49pt;text-indent: -43pt;text-align: justify;"><h2 style="display: inline;"><a name="bookmark11">Algorithms and Techniques</a></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark52" class="s13">Several models have been tried to approach this problem. Algorithms like Linear Discriminant Analysis (LDA), TreeBagger, Decision Trees and Support Vector Machines have been recently applied to the detection of up-calls [</a>10<span style=" color: #000;">].</span></p><p class="s5" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark46" class="s13">In order to perform the prediction, I will try to implement well-known and widely implemented models of ConvNets. ConvNets are outstanding at image detection but they do need large amount of data to train well enough and have high computational requirement. Up to now, ConvNets in acoustics have been used for the spectrogram, but I will try a new approach, to detect from the image of the spectrogram as human would do. I will try to check if the suitability of ConvNets for the detection of sound from the the image of the spectrogram [</a>4<span style=" color: #000;">].</span></p><p class="s5" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark53" class="s13">Convolutional Neural Networks (ConvNets) are a type of Neural Networks (NNs) that make the assumption that inputs are images. This allows to encode certain properties into the architecture that make the forward function more efficient to implement and vastly reduce the amount of parameters in the network [</a>11<span style=" color: #000;">]. ConvNets, like other NNs, are made up of layers. These layers</span></p><p class="s5" style="padding-top: 2pt;padding-left: 6pt;text-indent: -1pt;text-align: justify;"><a href="#bookmark53" class="s13">(called hidden layers) transform input 3D volumes to output 3D volumes with some differentiable function that may or may not have parameters [</a>11<a href="#bookmark16" class="s13">]. This is an interesting property of ConvNets: layers have neurons arranged in 3 dimensions (width, height and depth) (see figure </a><span style=" color: #EC008C;">6</span><span style=" color: #000;">).</span></p><p class="s5" style="padding-top: 6pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark53" class="s13">There are three main types of layers that are used to build ConvNets [</a>11<span style=" color: #000;">]:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l7"><li style="padding-left: 31pt;text-indent: -12pt;text-align: justify;"><p style="display: inline;">Convolutional Layer (CONV): computes the output of neurons connected to local regions of the input, performing the dot product between their weight and a small region that are connected to. If the input is 3D (i.e. an image with RGB colors) this layer will also be 3-dimensional. Four hyperparameters control the size of the output volume: the depth, filter size, stride, and zero-padding. Depth (D) is related to the number of filters used in the layer, the filter or patch have dimensions (F) (i.e. 2x2) and stride (S) is referred to the displacement of the filter. The amount Zero-padding (P), which allows controlling the spatial size of outputs.</p></li><li style="padding-top: 7pt;padding-left: 30pt;text-indent: -11pt;text-align: justify;"><p style="display: inline;">Pooling Layer (POOL) (or Subsampling Layer): performs downsampling operation along spatial dimensions. Can be max-pooling (taking the maximum of a region), average-pooling (taking the average of a region) or other types of results from applying a function to a region of the image. It has two main hyperparameters: the spatial extent of the filter or patch where pooling is applied (F) and the stride (S) of this filter.</p></li><li style="padding-top: 7pt;padding-left: 31pt;text-indent: -12pt;text-align: left;"><p style="display: inline;">Fully-Connected Layer (FC): is the same type of layer as in NNs.</p></li></ul><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;text-align: justify;">Usually, a CONV layer is followed by a Rectified Linear Unit (RELU) layer, which performs an element-wise activation function (i.e. max(0,x), logistic function, tanh).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 113pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="285" height="101" alt="image" src="capstone_report_mabelvj/Image_007.jpg"/></span></p><p class="s5" style="padding-top: 8pt;padding-left: 139pt;text-indent: 0pt;text-align: center;"><a href="#bookmark53" class="s13" name="bookmark16">Figure 6: ConvNet 3D structure [</a>11<span style=" color: #000;">]</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-left: 5pt;text-indent: 1pt;text-align: justify;"><a href="#bookmark17" class="s13">LeNet is one of the first successful applications of Convolutional Networks, developed by Yann LeCun in the 90s. One of the versions of LeNet is LeNet-5, which is highly used for handwritten and machine-printed character recognition. Figure </a>7 <span style=" color: #000;">shows the structure of the network, composed of 2 convolutional layers, 2 fully connected layers and 2 subsampling or pooling layers. The layers follow the sequence: INPUT -&gt; CONV -&gt; RELU -&gt; SUBS (POOL) -&gt; CONV -&gt; RELU -&gt; SUBS (POOL) -&gt; FC -&gt; RELU -&gt; FC, for INPUT, CONV, RELU, SUBS, FC.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="560" height="154" alt="image" src="capstone_report_mabelvj/Image_008.jpg"/></span></p><p class="s5" style="padding-top: 8pt;text-indent: 0pt;text-align: center;"><a href="#bookmark49" class="s13" name="bookmark17">Figure 7: LeNet-5 structure [</a>7<span style=" color: #000;">]</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-top: 7pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark50" class="s13">Other more complex ConvNet is AlexNet, developed by Alex Krizhevsky et al. [</a>8<span style=" color: #000;">]. The AlexNet was submitted to the ImageNet ILSVRC challenge in 2012 and significantly outperformed the second runner-up (top 5 error of 16% compared to runner-up with 26% error). It has a very similar</span></p><p class="s5" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark53" class="s13">architecture to LeNet, but it is deeper, bigger, and features Convolutional Layers stacked on top of each other (previously it was common to only have a single CONV layer always immediately followed by a POOL layer)[</a>11<a href="#bookmark18" class="s13">]. Figure </a><span style=" color: #EC008C;">8 </span><span style=" color: #000;">shows the structure of the network, composed of 5 convolutional layers, 3 fully connected layers and 3 subsampling or pooling layers. The layers follow the sequence: INPUT -&gt; CONV -&gt; RELU -&gt; SUBS -&gt; CONV -&gt; RELU -&gt; SUBS -&gt; CONV -&gt; RELU -&gt; CONV -&gt; RELU -&gt; SUBS -&gt; FC -&gt; RELU -&gt; FC.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="557" height="194" alt="image" src="capstone_report_mabelvj/Image_009.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 135pt;text-indent: 0pt;text-align: center;"><a href="#bookmark50" class="s13" name="bookmark18">Figure 8: AlexNet structure[</a>8<span style=" color: #000;">]</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-left: 6pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark28" class="s13">Some parameters need adjustment to adequate the network to this work, like the size of the patch (F) and stride (S) of the convolutional and pooling layers, and also the depth (D) of each convolutional layer. This issue will be discussed in later sections (see section </a>III.II<span style=" color: #000;">).</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-top: 8pt;padding-left: 49pt;text-indent: -42pt;text-align: justify;"><h2 style="display: inline;"><a name="bookmark12">Benchmark</a><a name="bookmark19">&zwnj;</a></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 5pt;text-indent: 1pt;text-align: justify;"><a href="#bookmark49" class="s13">I will try to compare the performance of popular ConvNets (i.e. LeNet-5 proposed by Lecun [</a>7<a href="#bookmark50" class="s13">] or AlexNet, the winner of the 2010 and 2012 ImageNet Large Scale Visual Recognition Competition (ILSVRC) proposed by Krizhevsky [</a>8<a href="#bookmark54" class="s13">, </a>12<a href="https://github.com/nmkridler/moby" class="s13" target="_blank">]) with the performance of the winning model of the competition which is based on Gradient Boosting (SluiceBox: </a>Github<a href="#bookmark50" class="s13">) and the Daniel Nouri’s model based on Krizhevsky’s 2012 ILSVRC ConvNet model [</a>8<a href="https://speakerdeck.com/dnouri/practical-deep-neural-nets-for-detecting-marine-mammals/" class="s13" target="_blank">] (</a>source<span style=" color: #000;">), which first inspired this work.</span></p><p class="s6" style="padding-top: 6pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark6" class="s13">The Area Under the Curve (AUC) (see the Evaluation metrics section </a><a href="#bookmark6" class="s4">I.I</a>II<span style=" color: #000;">) of these models in the public leaderboard was:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l8"><li style="padding-left: 31pt;text-indent: -12pt;text-align: left;"><p style="display: inline;">SluiceBox: 0.98410 (1st position)</p></li><li style="padding-top: 8pt;padding-left: 31pt;text-indent: -12pt;text-align: left;"><p style="display: inline;">Nouri: 0.98061 (6th position with 1/4 times the submission of the winner)</p></li></ul><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;text-align: justify;">Nevertheless, I will not be able to compare the performance of my models to these results. The reason is that I do not have the test labels and also, the public leaderboard data test used is slightly different for each participant. I will try two different approaches:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l9"><li style="padding-left: 31pt;text-indent: -12pt;text-align: left;"><p style="display: inline;">assuming that there are enough complete data samples (train dataset), trying to increase the accuracy as much as possible</p></li><li style="padding-top: 7pt;padding-left: 31pt;text-indent: -12pt;text-align: left;"><p style="display: inline;">assuming the predictions generated by the winning model as test labels and them as reference to compare our model with</p></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;text-align: justify;">Consequently, it will not be easy to truly compare the performance of this model with the ones in the competitions. Taking into consideration all the factors explained above, 0.95 AUC can be a really good value to achieve.</p></li></ol></li><li style="padding-top: 2pt;padding-left: 46pt;text-indent: -39pt;text-align: justify;"><h1 style="display: inline;"><a name="bookmark20">Methodology</a></h1><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l10"><li style="padding-left: 44pt;text-indent: -37pt;text-align: justify;"><h2 style="display: inline;"><a name="bookmark21">Data Preprocessing</a></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 6pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark55" class="s13">The processing of the images will consist in the usual mean subtraction and normalization (division by standard deviation) [</a>13<a href="#bookmark56" class="s13">]. The function used for this purpose is the StandardScaler from the preprocessing module from sklearn [</a>14<a href="#bookmark15" class="s13">]. The result of this process applied to images in figure </a><span style=" color: #EC008C;">5 </span><a href="#bookmark27" class="s13">is presented in figure </a><span style=" color: #EC008C;">9</span><span style=" color: #000;">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 49pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="457" height="343" alt="image" src="capstone_report_mabelvj/Image_010.jpg"/></span></p><p style="padding-top: 7pt;padding-left: 5pt;text-indent: 1pt;text-align: justify;"><a name="bookmark27">Figure 9: Samples after mean subtraction and division by standard deviation for right whale up-call (label 1) and no-right-whale up-call (label 0).</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-left: 6pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark15" class="s13">However, after applying this processing, images are not clear enough and energy where up-calls are contained is very low. In order to make them more visible, images in figure </a>5 <span style=" color: #000;">are first processed applying log10 and then  normalized  subtracting mean  and dividing  by the  standard  deviation. This method is widely used in spectrogram visualization when there is to rescale images, making minimum values more visible.</span></p><p style="padding-top: 6pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;">As it was commented previously, recordings have a duration of 1.8 seconds and a sampling rate of 2000Hz. The raw spectrogram of the recordings result in images of 129x23 pixels. Two problems arise from this: the excess of redundant information in frequencies not important for the detection of the up-call and the limitation in terms of the image of being processed by the network.</p><p style="padding-top: 6pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;">In order to solve the first problem, the frequency range will be limited 0-250 Hz, what will result in images of size 33x23 px. Still, this is not good for a ConvNet with lots of convolutions and pooling. It is necessary to resize the image and a good number is multiple of 2. Hence, I will choose images to be 32x32 px and will obtain additional pixels using interpolation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-top: 8pt;padding-left: 50pt;text-indent: -43pt;text-align: justify;"><h2 style="display: inline;"><a name="bookmark22">Implementation</a><a name="bookmark28">&zwnj;</a></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-left: 6pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark17" class="s13">The model implemented is LeNet-5 (see figure </a>7<span style=" color: #000;">). The only difference in the F6 layer, which has been removed. This layer is used for the detection of ASCII characters in 7x12 bitmaps, but this network does not intend to do so, it just needs to detect right-whale up-call or not.</span></p><p style="padding-top: 6pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;">The structure of the network is as follows (F= Filter, S= Stride, D= Depth):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 49pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="457" height="343" alt="image" src="capstone_report_mabelvj/Image_011.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 1pt;text-align: left;">Figure 10: Processed samples after mean subtraction and normalization for right whale up-call (label 1) and no-right-whale up-call (label 0).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 49pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="457" height="143" alt="image" src="capstone_report_mabelvj/Image_012.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Figure 11:  Samples reduced to 32x32 pixels after log10 and mean subtraction and normalization for right whale up-call (label 1) and no-right-whale up-call (label 0).</p><ul id="l11"><li style="padding-top: 2pt;padding-left: 30pt;text-indent: -12pt;text-align: left;"><p style="display: inline;">INPUT layer: 32x32 px image in gray scale represented with 8-bit number (0-255 levels).</p></li><li style="padding-top: 8pt;padding-left: 30pt;text-indent: -12pt;text-align: left;"><p style="display: inline;">C1 - CONV layer: F = 5x5, S = 1, D = 6</p></li><li style="padding-top: 8pt;padding-left: 30pt;text-indent: -12pt;text-align: left;"><p style="display: inline;">S2 - POOL layer: F = 2x2, S = 2, D = 6</p></li><li style="padding-top: 8pt;padding-left: 30pt;text-indent: -12pt;text-align: left;"><p style="display: inline;">C3 - CONV layer: F = 5x5, S = 1, D = 16</p></li><li style="padding-top: 8pt;padding-left: 30pt;text-indent: -12pt;text-align: left;"><p style="display: inline;">S4 - POOL layer: F = 2x2, S = 2, D = 16</p></li><li style="padding-top: 8pt;padding-left: 30pt;text-indent: -12pt;text-align: left;"><p style="display: inline;">C5 - CONV layer: F = 5x5, S = 1, D = 120</p></li><li style="padding-top: 8pt;padding-left: 30pt;text-indent: -12pt;text-align: left;"><p style="display: inline;">F5 - FC layer: neurons= 120 x 2, one per label(considering label 0 and label 1)</p></li></ul><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark57" class="s13">The training is performed using mini-batch gradient descent, which is a version of the true gradient descent (combines batch and stochastic gradient descent), used when the data amount is quite high. It iterates over batches of n samples in order to approach the minimum of the cost function step by step (epochs). Mini-batch gradient descent reduces the variance of the parameter updates, which can lead to more stable convergence. It also can make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient with respect to a mini-batch very efficient [</a>15<span style=" color: #000;">].</span></p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">The gradient descent main parameter is the learning rate (<i>α</i>). The learning rate expresses the speed of convergence of the gradient descent. Large learning rates lead to faster convergence but it may miss the minimum and not converge properly. Low learning rates lead to a better convergence point, but it is slower, requiring more steps and more memory allocation. A good compromise is to use a decaying learning rate: high values for the first epochs to accelerate the initial convergence and then smaller ones to slow it down.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="6" height="1" alt="image" src="capstone_report_mabelvj/Image_013.png"/></span></p><p class="s3" style="text-indent: 0pt;line-height: 7pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">Regularization is a common way to prevent over-fitting and the most used method is L2 regularization. This type of regularization penalizes the square magnitude of all parameters, adding the term <span class="s9">1</span><span class="s3"> </span><i>λω</i><span class="s2">2</span><span class="s3"> </span>to the prediction, for <i>λ </i><a href="#bookmark53" class="s13">the regularization strength [</a><span style=" color: #00F;">11</span>]. In this work L2 regularization will be used to control the over-fitting.</p><p class="s5" style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark58" class="s13">The network has been implemented making use of Tensorflow, using the basic of ConvNets explained in the Udacity Deep Learning Course [</a>16<span style=" color: #000;">] and then extending the functionality to more complex networks.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-top: 8pt;padding-left: 54pt;text-indent: -48pt;text-align: justify;"><h2 style="display: inline;"><a name="bookmark23">Refinement</a></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">The process followed has been one the iteration over different parameters to obtain the best combination.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 8pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a name="bookmark24">Select batch-size</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">In order to select the proper batch size simulations have been performed for different learning rates. After many simulations, I have observed that there must be a trade-off between batch-size and instability. The bigger the batch, the stabler the curves, but the poorer the performance since the number of epochs is smaller. The batch size must be big enough to provide a less noisy curve but small enough to give good values of prediction.</p><p class="s6" style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark29" class="s13">From figure </a>12a <span style=" color: #000;">it seems that a good compromise value is a batch size of 5.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="292" height="223" alt="image" src="capstone_report_mabelvj/Image_014.jpg"/></span>	<span><img width="273" height="219" alt="image" src="capstone_report_mabelvj/Image_015.jpg"/></span></p><p class="s8" style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><a name="bookmark29">(a) Error curve for the validation dataset, for different </a><span class="s10">(b)</span> AUC for the validation dataset, for different</p><p class="s8" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">batch sizes and a learning rate <i>α</i>=0.0005</p><p class="s8" style="padding-left: 6pt;text-indent: 33pt;line-height: 9pt;text-align: left;">learning rates and a batch size = 5</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Figure 12</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 6pt;text-indent: 0pt;text-align: justify;"><a name="bookmark25">Select learning rate</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-left: 6pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark29" class="s13">Secondly, for a fixed batch size , the cost has been calculated for different learning rates. Learning rates too big may not find the minimum and converge too fast. Small learning rate may be too slow and not fast enough for a small dataset like the one in this work. A compromise value must be chosen, showing a slope good that guarantees convergence. Figure </a>12b <span style=" color: #000;">shows that for the selected batch size of 5, a learning rate </span><span class="s7">α </span><a href="#bookmark30" class="s13">of 0.0005 seems like a good value. Figure </a>13 <span style=" color: #000;">shows that there is not over-fitting, since there is no gap between the training curve and the validation and test curves.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 102pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="310" height="252" alt="image" src="capstone_report_mabelvj/Image_016.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 75pt;text-align: left;"><a name="bookmark30">Figure 13: AUC for batch size = 5 and learning rate </a><i>α</i>=0.0005</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 6pt;text-indent: 0pt;text-align: justify;"><a name="bookmark26">Select regularization</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-left: 6pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark31" class="s13">Finally, for fixed batch size and learning rate, the AUC has been calculated for regularization parameters. Regularization is a good way to limit over-fitting, allowing the model to generalize better. Figure </a>14 <a href="#bookmark31" class="s13">shows the AUC curve for different values of the regularization parameters. Figure </a>14a <a href="#bookmark31" class="s13">shows that a good value for the regularization parameter can is near 0.95, and figure </a>14b <span style=" color: #000;">shows that for the selected batch size of 5 and learning rate of 0.0005 a good regularization value is 0.97, since it allows to reach a greater AUC.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="281" height="236" alt="image" src="capstone_report_mabelvj/Image_017.jpg"/></span>	<span><img width="284" height="223" alt="image" src="capstone_report_mabelvj/Image_018.jpg"/></span></p><p class="s8" style="padding-top: 7pt;padding-left: 4pt;text-indent: 0pt;text-align: center;"><a name="bookmark31">(a)</a></p><p class="s8" style="padding-top: 1pt;text-indent: 0pt;text-align: center;">(b)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">Figure 14: AUC in the validation dataset for different regularization values, batch size = 5 and learning rate <i>α</i>=0.0005</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li style="padding-top: 8pt;padding-left: 44pt;text-indent: -38pt;text-align: justify;"><h1 style="display: inline;"><a name="bookmark32">Results</a></h1><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l12"><li style="padding-left: 43pt;text-indent: -37pt;text-align: justify;"><h2 style="display: inline;"><a name="bookmark33">Model Evaluation and Validation</a><a name="bookmark35">&zwnj;</a></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark49" class="s13">The model used is the LeNet-5 [</a><span style=" color: #00F;">7</span><a href="#bookmark36" class="s13">]. Figure </a>15 <span style=" color: #000;">shows the AUC and the error for different epochs, achieving 0.944 AUC in the test set (and up to 0.958 depending on the test split) and an error of 12% when reaching 2500 epochs, batch size = 5, learning rate </span><span class="s7">α</span><a href="#bookmark36" class="s13">=0.0005 and regularization parameter = 0.97. Figure </a>15 <a href="#bookmark13" class="s13">shows there is no over-fitting, since validation and test curve follow the training curve. This is clearly a good result given the limitations related to data in this work previously explained in section </a>II.I <a href="#bookmark19" class="s13">and is very close to the objective of 0.95 AUC stated in </a>II.IV<span style=" color: #000;">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="285" height="225" alt="image" src="capstone_report_mabelvj/Image_019.jpg"/></span> <span><img width="280" height="219" alt="image" src="capstone_report_mabelvj/Image_020.jpg"/></span></p><ol id="l13"><li style="padding-top: 3pt;padding-left: 322pt;text-indent: -216pt;text-align: left;"><p class="s12" style="display: inline;"><a name="bookmark36">(b)</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">Figure 15: (a) AUC and (b) error for the train validation and test dataset, for the final model batch size = 5, learning rate <i>α</i>=0.0005and regularization parameter = 0.97</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">One problem I faced when training the model was the limitation in terms of data samples for training which reduced the maximum AUC obtained. In order to solve it, labels for the test dataset have been obtained using the evaluations from the winning model as true labels (SluiceBox: 0.98410 AUC). This has allowed o increase the volume of data samples available to train the model and hence, increase its performance.  I have taken this decision after taking into consideration the</p><p class="s5" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark46" class="s13">processed of obtaining the samples. The sounds were firstly recorded when a buoy detected an up-all in the area. Afterwards, samples were labeled by human ear. This leads to a lot of mislabeled samples (as mentioned in [</a>4<span style=" color: #000;">]), and gives the intuition that using the labels from the prediction of a good model as test dataset, freeing samples for training, can be more good than harm. This has allowed to extend the available data for training (from around 8000 to 12500 samples from the balanced dataset of 14000 samples) and have available 54503 samples from the unbalanced testing dataset for testing, though it was used 10000 random samples for computational issues. This dataset was not artificially balanced just to try how the training would perform in a real world with little samples from one label when compared to the other.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li style="padding-top: 6pt;padding-left: 48pt;text-indent: -42pt;text-align: justify;"><h2 style="display: inline;"><a name="bookmark34">Justification</a></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">This work has shown the use of simple ConvNet for audio recognition, obtaining good performance with an AUC of up to 0.958, whereas more complex models as SluiceBox obtained 0.98410 AUC and Nuori 0.98061.</p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">One of the advantages of simpler networks relies upon the reduction of time required for training the model, resulting in fewer computer requirements. With a vast dataset, the system would increase significantly the performance and equal those with more complex structure, making it a simple, yet robust choice.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li style="padding-left: 38pt;text-indent: -32pt;text-align: justify;"><h1 style="display: inline;"><a name="bookmark37">Conclusion</a></h1><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l14"><li style="padding-left: 38pt;text-indent: -32pt;text-align: justify;"><h2 style="display: inline;"><a name="bookmark38">Free-Form  Visualization</a></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark41" class="s13">In this section I will try to give an insight of the network, presenting, as it is frequently done, the characteristics f the first layer. Figure </a>16a <span style=" color: #000;">it is presented the evolution of the filters of the first CONV layer (F=5x5) with depth 6 (D=6). These filters are the ones convoluting and sweeping through the image and contain the weights fond the CONV layer neurons. It is clear how, due to backpropagation and minimization of the cost function, weights are changed and they evolve to new and more adequate values for prediction. In fact, the six filters start very undefined and they adapt to their final value.</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="284" height="198" alt="image" src="capstone_report_mabelvj/Image_021.jpg"/></span></p><p class="s6" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark41" class="s13">Figure </a>16b <span style=" color: #000;">shows the evolution of the biases with the number of epochs and how they tend to stabilize. These biases are the biases of the CONV layer and they change to minimize the cost function.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 45pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="171" height="172" alt="image" src="capstone_report_mabelvj/Image_022.jpg"/></span></p><p class="s8" style="padding-top: 4pt;padding-left: 43pt;text-indent: 0pt;text-align: center;"><a name="bookmark41">(a)</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="text-indent: 0pt;text-align: center;">(b)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Figure 16: (a) Weights and (b) biases of the first layer with 6 filters of depth every 500 epochs for batch size=5, learning rate <i>α</i>=0.0005and regularization parameter= 0.97</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark42" class="s13">Finally, figure </a>17 <span style=" color: #000;">shows the output every 500 epochs for some samples. It is not clear, but in some of the squares it can be seen the shape of a the up-call.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 133pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span><img width="228" height="228" alt="image" src="capstone_report_mabelvj/Image_023.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a name="bookmark42">Figure 17: Activation of the first layer with 6 filters of depth every 500 epochs for batch size=5 and learning rate </a><i>α</i>=0.0005</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-top: 8pt;padding-left: 43pt;text-indent: -37pt;text-align: justify;"><h2 style="display: inline;"><a name="bookmark39">Reflection</a></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">In this project, it is presented how simple ConvNets can be used to label data from the image of the spectrogram, as it in many classification problems. More complex networks may achieve better results, but for an image of 32x32px, I prefer to take the strategy: the simpler, the better.</p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">In this case, the use of LeNet-5 has given a 0.44-0.958 AUC, which is really good given the low complexity of the network.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-top: 8pt;padding-left: 48pt;text-indent: -43pt;text-align: justify;"><h2 style="display: inline;"><a name="bookmark40">Improvement</a></h2></li></ol></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">In this work, the problem is to recognize a simple and very specific spectrogram pattern from in a form of an image. A good rule of thumb for this is the simpler, the better and LeNet-5 achieves good performance with a simple structure.</p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">The complexity of the model should not be the focus, since like in any Machine Learning problem, the most important factor is the amount of data. Collecting more samples and extending the training would make LeNet-5 a good competitor against other models achieving better performance with over-complicated models.</p><p class="s5" style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark50" class="s13">Once resolved the issue with the amount of data, more complex models are good start points to increase the performance and achieve better results and compare the trade-off between complexity with results. Good examples of models to try are AlexNet [</a>8<a href="#bookmark59" class="s13">] or GoogLeNet [</a>17<a href="#bookmark60" class="s13">]. Dropout has not been applied in this work, since the network has lots of max-pooling layers and regularization, and in these cases, the use of dropout is not clear. Introducing dropout could be a good option to try. Also, methods to make faster training, as Batch Normalization may lead to better results [</a>18<span style=" color: #000;">].</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">References</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 10pt;text-indent: 0pt;text-align: justify;"><a href="https://www.kaggle.com/c/whale-detection-challenge" class="s13" target="_blank" name="bookmark43">[1]  Kaggle.  The Marinexplore and Cornell University Whale Detection Challenge.  URL </a><a href="https://www.kaggle.com/c/whale-detection-challenge" class="a" target="_blank">https:</a></p><p class="s5" style="padding-left: 25pt;text-indent: 0pt;text-align: left;">//www.kaggle.com/c/whale-detection-challenge<span style=" color: #000;">.</span></p><p style="padding-top: 8pt;padding-left: 10pt;text-indent: 0pt;text-align: justify;"><a name="bookmark44">[2]  Cornell Bioacoustics Research Program.  Right Whale’s Up-Call, Cornell Bioacoustics Resear, .</a></p><p class="s5" style="padding-left: 26pt;text-indent: 0pt;text-align: left;"><a href="http://www.listenforwhales.org/page.aspx?pid=432" class="s13" target="_blank">URL    </a>http://www.listenforwhales.org/page.aspx?pid=432<span style=" color: #000;">.</span></p><p class="s5" style="padding-top: 8pt;padding-left: 26pt;text-indent: -15pt;text-align: left;"><a href="http://www.listenforwhales.org/page.aspx?pid=442" class="s13" target="_blank" name="bookmark45">[3]  Cornell  Bioacoustics  Research  Program.    More  Right  Whale  calls,  .    URL  </a><a href="http://www.listenforwhales.org/page.aspx?pid=442" class="a" target="_blank">http://www. </a>listenforwhales.org/page.aspx?pid=442<span style=" color: #000;">.</span></p><p style="padding-top: 2pt;padding-left: 10pt;text-indent: 0pt;text-align: left;"><a href="http://danielnouri.org/notes/2014/01/10/using-deep-learning-to-listen-for-whales/" class="s13" target="_blank" name="bookmark46">[4] Daniel Nouri. Using deep learning to listen for whales — Daniel Nouri’s Blog. URL </a><a href="http://danielnouri.org/notes/2014/01/10/using-deep-learning-to-listen-for-whales/" class="a" target="_blank">http:</a></p><p class="s5" style="padding-left: 25pt;text-indent: 0pt;text-align: left;">//danielnouri.org/notes/2014/01/10/using-deep-learning-to-listen-for-whales/<span style=" color: #000;">.</span></p><p style="padding-top: 8pt;padding-left: 10pt;text-indent: 0pt;text-align: left;"><a href="https://en.wikipedia.org/wiki/Receiver%7B_%7Doperating%7B_%7Dcharacteristic" class="s13" target="_blank" name="bookmark47">[5]  Wikipedia;  the free encyclopedia.   Receiver Operating Characteristic  (ROC).   URL </a><a href="https://en.wikipedia.org/wiki/Receiver%7B_%7Doperating%7B_%7Dcharacteristic" class="a" target="_blank">https:</a></p><p class="s5" style="padding-left: 25pt;text-indent: 0pt;text-align: left;"><a href="https://en.wikipedia.org/wiki/Receiver%7B_%7Doperating%7B_%7Dcharacteristic" class="a" target="_blank">//en.wikipe</a>dia.org/wiki/Receiver{_}operating{_}characteristic<span style=" color: #000;">.</span></p><p class="s5" style="padding-top: 8pt;padding-left: 26pt;text-indent: -15pt;text-align: justify;"><a href="http://www.wikiwand.com/it/Receiver%7B_%7Doperating%7B_%7Dcharacteristic" class="s13" target="_blank" name="bookmark48">[6] Wikiwand. Receiver operating characteristic. URL </a><a href="http://www.wikiwand.com/it/Receiver%7B_%7Doperating%7B_%7Dcharacteristic" class="a" target="_blank">http://www.wikiwand.com/it/ Receiv</a>er{_}operating{_}characteristic<span style=" color: #000;">.</span></p><p style="padding-top: 7pt;padding-left: 25pt;text-indent: -15pt;text-align: justify;"><a name="bookmark49">[7] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.  Gradient-based learning applied to document recognition. </a><i>Proc. IEEE</i>, 86(11):2278–2323, 1998. ISSN 00189219. doi: 10.1109/5.726791.</p><p style="padding-top: 7pt;padding-left: 25pt;text-indent: -15pt;text-align: justify;"><a name="bookmark50">[8] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet Classification with Deep Convolutional Neural Networks. </a><i>Adv. Neural Inf. Process. Syst.</i><a href="http://dx.doi.org/10.1016/j.protcy.2014.09.007" class="s13" target="_blank">, pages 1–9, 2012. ISSN 10495258.  doi:  http://dx.doi.org/10.1016/j.protcy.2014.09.007.</a></p><p style="padding-top: 7pt;padding-left: 25pt;text-indent: -15pt;text-align: justify;"><a name="bookmark51">[9] M A McDonald and S E Moore. Calls recorded from North Pacific right whales (&lt;i&gt;Eubalaena japonica&lt;/i&gt;) in the eastern Bering Sea. </a><i>J. Cetacean Res. Manag.</i><a href="http://www.afsc.noaa.gov/nmml/PDF/rightcalls.pdf" class="s13" target="_blank">, 4(3):261–266, 2002. ISSN 1561-0713.    URL   </a><a href="http://www.afsc.noaa.gov/nmml/PDF/rightcalls.pdf" class="a" target="_blank">http://www.afsc.noaa.gov/nmml/PDF/rightcalls.pd</a><span style=" color: #00F;">f</span>.</p><p style="padding-top: 7pt;padding-left: 26pt;text-indent: -20pt;text-align: justify;"><a name="bookmark52">[10] Mahdi Esfahanian, Hanqi Zhuang, Nurgun Erdol, Edmund Gerstein, Computer Science, and Boca Raton. Comparison of Two Methods for Detection of North Atlantic Right. </a><i>Ieee</i>, pages 559–563, 2015.</p><p class="s5" style="padding-top: 7pt;padding-left: 26pt;text-indent: -20pt;text-align: justify;"><a href="http://cs231n.github.io/" class="s13" target="_blank" name="bookmark53">[11] Johnson Justin Karpathy Andrej. CS231n Convolutional Neural Networks for Visual Recogni- tion,  .   URL  </a>http://cs231n.github.io/<span style=" color: #000;">.</span></p><p style="padding-top: 7pt;padding-left: 26pt;text-indent: -20pt;text-align: justify;"><a name="bookmark54">[12] Alex Krizhevsky and G Hinton. Convolutional deep belief networks on cifar-10. </a><i>Un- publ. Manuscr.</i><a href="http://scholar.google.com/scholar?hl=en%7B&amp;%7DbtnG=Search%7B&amp;%7Dq=intitle%3AConvolutional%2BDeep%2BBelief%2BNetworks%2Bon%2BCIFAR-10%7B&amp;%7D0" class="s13" target="_blank">, pages 1–9, 2010. URL </a><a href="http://scholar.google.com/scholar?hl=en%7B&amp;%7DbtnG=Search%7B&amp;%7Dq=intitle%3AConvolutional%2BDeep%2BBelief%2BNetworks%2Bon%2BCIFAR-10%7B&amp;%7D0" class="a" target="_blank">http://scholar.google.com/scholar?hl=en{&amp;}btnG= Search{&amp;}q=intitle:Convolutional+Deep+Belief+Networks+on+CIFAR-</a><span style=" color: #00F;">10{#}0</span>.</p><p class="s5" style="padding-top: 7pt;padding-left: 26pt;text-indent: -20pt;text-align: justify;"><a href="http://cs231n.github.io/neural-networks-2/" class="s13" target="_blank" name="bookmark55">[13] Johnson Justin Karpathy Andrej. CS231n Convolutional Neural Networks for Visual Recogni- tion,  .   URL  </a><a href="http://cs231n.github.io/neural-networks-2/" class="a" target="_blank">http://cs231n.github.io/neural-netw</a>orks-2/<span style=" color: #000;">.</span></p><p class="s5" style="padding-top: 7pt;padding-left: 26pt;text-indent: -20pt;text-align: justify;"><a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" class="s13" target="_blank" name="bookmark56">[14] sklearn.preprocessing.StandardScaler — scikit-learn 0.18 documentation. URL </a><a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" class="a" target="_blank">http:// </a>scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html<span style=" color: #000;">.</span></p><p style="padding-top: 7pt;padding-left: 26pt;text-indent: -20pt;text-align: justify;"><a name="bookmark57">[15] Sebastian Ruder. An overview of gradient descent optimization algorithms. sep 2016. URL </a><a href="http://sebastianruder.com/optimizing-gradient-descent/" class="a" target="_blank">h</a><span style=" color: #00F;">ttp://arxiv.org/abs/1609.04747http://sebastianruder.com/optimizing-gradient-descent/ index.html{#}gradientdescentvariants</span>.</p><p class="s5" style="padding-top: 7pt;padding-left: 26pt;text-indent: -20pt;text-align: justify;"><a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/udacity" class="s13" target="_blank" name="bookmark58">[16] Udacity. Deep Learning Course Repository. URL </a><a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/udacity" class="a" target="_blank">https://github.com/tensorflow/tensorflow/ tree/master/tensorflow/examples/udacit</a>y<span style=" color: #000;">.</span></p><p style="padding-top: 7pt;padding-left: 26pt;text-indent: -20pt;text-align: justify;"><a name="bookmark59">[17] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In </a><i>Proc. IEEE Comput. Soc. Conf. Comput. Vis.  Pattern  Recognit.</i><a href="http://arxiv.org/abs/1409.4842" class="s13" target="_blank">,  volume  07-12-June, pages 1–9, sep 2015.  ISBN 9781467369640.  doi:  10.1109/CVPR.2015.7298594.  URL </a><a href="http://arxiv.org/abs/1409.4842" class="a" target="_blank">http:</a></p><p class="s5" style="padding-left: 25pt;text-indent: 0pt;text-align: left;">//arxiv.org/abs/1409.4842<span style=" color: #000;">.</span></p><p style="padding-top: 8pt;padding-left: 26pt;text-indent: -20pt;text-align: justify;"><a name="bookmark60">[18] Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. </a><i>arXiv:1502.03167</i><a href="http://arxiv.org/abs/1502.03167" class="s13" target="_blank">, pages 1–11, 2015. ISSN 0717-6163. doi:  10.1007/s13398-014-0173-7.2.  URL </a><span style=" color: #00F;">http://arxiv.org/abs/1502.03167</span>.</p></body></html>
